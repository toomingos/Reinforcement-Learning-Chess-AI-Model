# Building a Chess AI Model with Reinforcement Learning

## Introduction
Due to the chess environment's complex nature and the pure number of possible states, the creation of a Reinforcement Learning model to compete against Stockfish, one of the strongest chess engines in the world, presents an unusual challenge. Chess has an exorbitantly large number of possible states, in contrast to many conventional tabular environments. The branching factor at each state is typically between 25 and 40 moves1 per play (the average is around 35), resulting in an estimated total number of states on the order of 10^402. 
Since it is impossible to explore every possible state, we choose to infer on model free based Reinforcement Learning algorithms, and how they would compete with the Stockfish environment. The models used were Monte Carlo and Q-Learning. We trained models with themselves, so they could learn and develop their functions in different scenarios, before facing the Stockfish environment model.
## Models
### Monte Carlo
In order to make the best decisions possible in unknowable environments, Monte Carlo reinforcement learning is an algorithm that combines the fundamentals of support education with macho simulation. The realtor interacts with the environment, gathering knowledge through state-action sequence episodes and receiving rewards in the form of feedback. The algorithm estimates the anticipated returns for each state-action pair and adjusts the owner's policy by repeating this process and averaging the rewards received from various episodes. The agent slowly develops its decision-making skills and converges towards an ideal policy that maximizes long-term rewards through iterative updates and learning from sampled experiences.
### Q-Learning
Q-Learning is a model-free, off-policy reinforcement learning algorithm used for decision-making in an unknown environment. It involves learning from experiences and improving actions over time.  It uses the Q-function, which represents the expected future rewards for taking a particular action in a given state. Q-learning is an algorithm used in reinforcement learning to make intelligent decisions. 
The choice of Q-Learning as one of the models is justified due to its effectiveness in dealing with unfamiliar environments and decision-making tasks.  Furthermore, Q-Learning's ability to learn from experiences and improve actions over time aligns with our goal of training an intelligent agent through reinforcement learning. By using the Q-function to estimate future rewards, Q-Learning allows the agent to make informed decisions based on maximizing the accumulated rewards over the long term. Overall, Q-Learning provides a flexible and adaptive approach to our problem, making it a suitable choice for our decision-making tasks in an unfamiliar environment.
## Training Models
In this section, we will discuss the approaches and constraints implemented to ensure effective training of our models. Our objective was to find a balance between computational resources, fairness, and adaptability. To prevent prolonged repetitive moves without progress, we set a maximum iteration limit of 100 moves during training. If the game did not conclude within this limit, the player closer to winning was declared the victor. Additionally, to promote fairness and adaptability, we alternated the sides between white and black pieces in the training games. These measures were implemented to optimize the training process and enhance the learning capabilities of our models. Later on we reduced the moves limit to 20 so we could iterate through our approach faster and conclude more efficiently to what changes had a bigger impact. Furthermore, we made adjustments to the reward system during the course of the project, which will be discussed in detail below.
## Model VS Random Actions
### Q-Learning
We started by training each model by assigning an opponent who would act randomly. In this application, the reward mechanism is only propagated after the game is finished and its value was calculated based on the results of losing or winning. Surprisingly, despite the fact that we repeated it several times, giving the model functions time to develop, the most frequent outcome was a draw, occupying a portion of 96% of the games, while lossed represented 4%. That said, we concluded that this was not the best approach.
We later changed the reward feeding mechanism, where previously our model received a reward only after the outcome of a game, to the model being fed a reward whenever each chess piece is removed from the game by our agent or the opponent, i.e. a reward, negative or positive, was propagated to our models. After this change was implemented, we saw an incredible improvement in the pace of development, as our model started winning about half the time (49%) on the white side but on the black side showing a defeat percentage of 93%. The new reward philosophy was the following, neutral moves (no pieces removed): 0; if our model removed an opponents chess piece: +weight of the piece; If our agent had a chess piece removed from the game: -weight of the piece.. Our model in the last episode had accumulated a total of 17 reward values by the end of the last episode1. Here you may observe weights of pieces: {'♛':5,'♜':3,'♝':2,'♞':2,'♟':1}
### Monte Carlo
	We repeated the approach applied with Q-Learning, on the first iteration, where our model ran against an opponent that played with random action. After running our model, who learnt at the end of each episode, we witnessed poor results as our model failed to win a game, drawing 98% of the games and losing 2%.
	After changing the reward mechanism so it would get feedback in the form of rewards after each piece in the game was eliminated, we observed a staggering increase in rewards aggregate. With the new parameters, our model beat it’s opponent 53% of times, drawing 8% and losing 39%. The rewards sum in the last episode also ranged, after the iterations, in between 4 and -4, meaning there were momentum swings though the episode.
## Model VS Model
For the two models, we adopted a training strategy that had them playing against each other to optimize their performance over time. We organized several games in which the models played alternately in black and white. This allowed us to gather a diverse set of experiences from different perspectives. Through repeated iterations and training episodes, the models gradually improved their decision-making skills, discovering more effective strategies and counter-strategies, so that they were in their best shape to take on the chess engine Stockfish.
During the training process, we highlighted the number of draws when the models were pitted against each other. A large number of draws occurs due to factors such as balance in terms of competence and strategy. This balance leads to a higher probability of draws as both models make comparable moves and counter effectively. Furthermore, the exploration phase of training makes models need to explore different strategies and actions to learn effectively and thus this exploration may result in suboptimal moves or even a cautious mode of play, which promotes the number of draws as models prioritize learning over winning. Another contributing factor is the defensive play adopted by the models. These may adopt more conservative strategies to minimize the risk of losing, which results in longer games and a greater likelihood of draws. Finally, the symmetry that characterizes chess means that both models have similar opportunities to effectively counter each other's moves, which leads to a higher probability of draws. 
Stockfish as Opponent
Results and Discussion
### Monte Carlo VS Stockfish
	At this stage we took our Monte Carlo model to play against the Stockfish environment for 30 episodes, where our model won 30% of the game, lost 63.33% and drew 6.67%. Monte Carlo won in the last episode and had a total rewards sum of 1.5.
Q-Learning VS Stockfish
	After applying the trained models against the Stockfish environment, after 30 episodes, we observed 46.67% wins against the opponent, 10% draws and 43.33% losses, the highest we have gotten. Fair to say the difficulty parameter of the Stockfish was set to 100, which is not the highest, although we’re happy with the results. At the last episode our model had a total rewards sum equal to 1.

## Conclusion
In conclusion, we experienced that how we structure the reward mechanism is how we guide the learning path of the model itself. The exponential growth in the success of our models once we gave rewards after removing chess pieces instead of strictly rewarding them for the end of episode result was the decision in this project with the biggest impact. In the future, we now know that how we reward the model might just be the most crucial point in order to succeed in all the other details of Reinforcement Learning model training as well.
## References
1. AI Chess Algorithms. Cornell, https://www.cs.cornell.edu/boom/2004sp/ProjectArch/Chess/chessreport.html. Accessed 24 June 2023.
2. “Which Is Greater? The Number of Atoms in the Universe or the Number of Chess Moves?” National Museums Liverpool, https://www.liverpoolmuseums.org.uk/stories/which-greater-number-of-atoms-universe-or-number-of-chess-moves. Accessed 24 June 2023.
